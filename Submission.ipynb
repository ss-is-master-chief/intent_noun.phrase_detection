{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T17:24:51.234863Z",
     "start_time": "2018-12-30T17:24:46.929730Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "huJYeymZGD_K",
    "outputId": "c8c960ce-016f-44e2-a67d-903dfebe5fd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sumitsaha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sumitsaha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/sumitsaha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json\n",
    "from keras import backend\n",
    "\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "The `clean_data()` method reads the `file_name` passed onto it and does the following to each line of text\n",
    "* Split the decision - `Yes` or `No` by `\\t` within a sentence and the lines by `\\n`\n",
    "* Converts the contents to lower case\n",
    "* Expands contractions, e.g. converts `you've` to `you have`\n",
    "* Removes hyperlinks since they can be arbitrary which might throw our model off track\n",
    "* Removes email addresses \n",
    "* Removes numbers\n",
    "* Removes punctuations\n",
    "* Assigns integer value of `1` to `yes` and `0` to `no` for classification purposes\n",
    "* Extracts noun-phrases from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T17:24:51.307278Z",
     "start_time": "2018-12-30T17:24:51.237439Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "sfUKwNttGT9b"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "clean_data() returns the feature set, decision \n",
    "and the associated noun-phrases\"\n",
    "'''\n",
    "def clean_data(file):\n",
    "    file = open(file, 'r', encoding=\"utf-8\")\n",
    "    corpus = file.read()\n",
    "\n",
    "    # list of stopwords from the english language\n",
    "    stop_words = stopwords.words('english')\n",
    "    # retrieving punctuations from string module\n",
    "    punctuations = [i for i in string.punctuation]\n",
    "\n",
    "    # splitting the data into (X,Y) for training/testing purposes by \"\\t\"\n",
    "    # splitting the corpus into observations by \"\\n\" \n",
    "    data = [i.split(\"\\t\") for i in corpus.split(\"\\n\")]\n",
    "    \n",
    "    # decision vector - yes or no\n",
    "    target = [t[0].lower() for t in data]\n",
    "    # associated text\n",
    "    text = [contractions.fix(t[1].lower()) for t in data]\n",
    "\n",
    "    # ---- Extracting Noun-Phrases from text ----\n",
    "    \n",
    "    # \"noun_phrases\" contains individual lists of noun-phrases from all sentences\n",
    "    noun_phrases = list()\n",
    "    for sentence in text:\n",
    "        # \"per_noun_phrases\" contain all noun-phrases from each sentence\n",
    "        per_noun_phrases = list()\n",
    "        # removing all integers from sentences\n",
    "        sentence = re.sub(r'\\d+', \"\", sentence)\n",
    "        # taking care of residual spaces after digit removal\n",
    "        sentence = sentence.replace(\"  \", \" \")\n",
    "        # tokenizing the sentence into words\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        # empty string \"np\" will take in individual noun-phrases\n",
    "        np = ''\n",
    "    \n",
    "        for w in words:\n",
    "            if w not in stop_words+punctuations:\n",
    "                np += w + ' '\n",
    "            else:\n",
    "                if np != '':\n",
    "                    per_noun_phrases.append(np.strip())\n",
    "                    np = ''\n",
    "        noun_phrases.append(per_noun_phrases)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    text = [re.sub(r\"http\\S+\", \"\",t[1].lower()) for t in data]\n",
    "    # remove email addresses\n",
    "    text = [re.sub(r\"[^@]+@[^@]+\\.[^@]+\", \"\", t) for t in text]\n",
    "\n",
    "    # removing blank observations \n",
    "    df = pd.DataFrame( {'text': text, 'target': target})\n",
    "    df = df[df.iloc[:,0] != '']\n",
    "    text = df['text'].values.tolist()\n",
    "    target = df['target'].values.tolist()\n",
    "\n",
    "    # assigning unique indices to 'yes' and 'no'\n",
    "    for x in target:\n",
    "        if(x==\"no\"):\n",
    "            target[target.index(x)] = 0\n",
    "        else:\n",
    "            target[target.index(x)] = 1\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for x in text:\n",
    "        tokens = word_tokenize(x)\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # removing punctuation\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove numbers\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [lemmatizer.lemmatize(w) for w in words if not w in stop_words]\n",
    "        text[text.index(x)] = words\n",
    "\n",
    "    return text, target, noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T17:25:00.501051Z",
     "start_time": "2018-12-30T17:24:51.310442Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oA36QP2bGuEI"
   },
   "outputs": [],
   "source": [
    "# retrieving cleaned data and noun-phrases \n",
    "# from train and test datasets\n",
    "text_train, target_train, noun_phrases_train = clean_data(\"./dataset/train.txt\")\n",
    "text_test, target_test, noun_phrases_test = clean_data(\"./dataset/test.txt\")\n",
    "\n",
    "# all data tokenized\n",
    "# converting back to cleaned sentences\n",
    "train = [' '.join(x) for x in text_train]\n",
    "test = [' '.join(x) for x in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Word Embeddings using FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T17:25:12.687625Z",
     "start_time": "2018-12-30T17:25:00.523225Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NitOKBruHRv9"
   },
   "outputs": [],
   "source": [
    "# training word embeddings from a training corpus \n",
    "# additional ability to obtain word vectors for out-of-vocabulary words.\n",
    "model_ted = FastText(text_train + text_test, size=1000, window=5, min_count=5, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T17:25:12.807022Z",
     "start_time": "2018-12-30T17:25:12.689387Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "K_oOArXAIshu"
   },
   "outputs": [],
   "source": [
    "# 10,000 should be enough number of vocabulary items\n",
    "vocab_size = 10000\n",
    "# combines the train and test set\n",
    "# finds one-hot vector of every word\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in train+test]\n",
    "\n",
    "# sequences are not of equal lengths\n",
    "# keras requires all vectors to be of equal length\n",
    "max_length = 131\n",
    "padded_docs = pad_sequences(encoded_docs[:len(train)], maxlen=max_length)\n",
    "padded_docs_test = pad_sequences(encoded_docs[len(train):], maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T17:26:05.188312Z",
     "start_time": "2018-12-30T17:25:12.808889Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674
    },
    "colab_type": "code",
    "id": "MNLOJx3FIw-6",
    "outputId": "8a89da62-35cc-4267-b115-256b3a76f20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU runtime...\n",
      "Epoch 1/18\n",
      "3577/3577 [==============================] - 0s 68us/step - loss: 0.2499 - acc: 0.5205\n",
      "Epoch 2/18\n",
      "3577/3577 [==============================] - 0s 25us/step - loss: 0.2491 - acc: 0.5281\n",
      "Epoch 3/18\n",
      "3577/3577 [==============================] - 0s 28us/step - loss: 0.2484 - acc: 0.5292\n",
      "Epoch 4/18\n",
      "3577/3577 [==============================] - 0s 28us/step - loss: 0.2473 - acc: 0.5398\n",
      "Epoch 5/18\n",
      "3577/3577 [==============================] - 0s 25us/step - loss: 0.2458 - acc: 0.5544\n",
      "Epoch 6/18\n",
      "3577/3577 [==============================] - 0s 31us/step - loss: 0.2434 - acc: 0.5686\n",
      "Epoch 7/18\n",
      "3577/3577 [==============================] - 0s 26us/step - loss: 0.2409 - acc: 0.5801\n",
      "Epoch 8/18\n",
      "3577/3577 [==============================] - 0s 23us/step - loss: 0.2375 - acc: 0.5927\n",
      "Epoch 9/18\n",
      "3577/3577 [==============================] - 0s 24us/step - loss: 0.2329 - acc: 0.6201\n",
      "Epoch 10/18\n",
      "3577/3577 [==============================] - 0s 20us/step - loss: 0.2271 - acc: 0.6461\n",
      "Epoch 11/18\n",
      "3577/3577 [==============================] - 0s 22us/step - loss: 0.2203 - acc: 0.6791\n",
      "Epoch 12/18\n",
      "3577/3577 [==============================] - 0s 23us/step - loss: 0.2124 - acc: 0.7062\n",
      "Epoch 13/18\n",
      "3577/3577 [==============================] - 0s 21us/step - loss: 0.2042 - acc: 0.7269\n",
      "Epoch 14/18\n",
      "3577/3577 [==============================] - 0s 20us/step - loss: 0.1964 - acc: 0.7380\n",
      "Epoch 15/18\n",
      "3577/3577 [==============================] - 0s 21us/step - loss: 0.1892 - acc: 0.7498\n",
      "Epoch 16/18\n",
      "3577/3577 [==============================] - 0s 23us/step - loss: 0.1829 - acc: 0.7487\n",
      "Epoch 17/18\n",
      "3577/3577 [==============================] - 0s 21us/step - loss: 0.1772 - acc: 0.7554\n",
      "Epoch 18/18\n",
      "3577/3577 [==============================] - 0s 23us/step - loss: 0.1719 - acc: 0.7657\n",
      "\n",
      "992/992 [==============================] - 0s 69us/step\n",
      "Accuracy: 81.25 %\n",
      "\n",
      "Save Model? (yes=1, no=otherwise): 1\n",
      "Save as: model\n",
      "Saved model.json and model.h5 to disk\n"
     ]
    }
   ],
   "source": [
    "# to ensure reproducible results\n",
    "seed(1)\n",
    "if(len(backend.tensorflow_backend._get_available_gpus())==0):\n",
    "    print(\"CPU runtime...\")\n",
    "    set_random_seed(3)\n",
    "else:\n",
    "    print(\"GPU runtime...\")\n",
    "    set_random_seed(2)\n",
    "\n",
    "# establishing a neural-network\n",
    "# experimentation led to 18 epochs\n",
    "# avoiding over-fitting and under-fitting\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "'''\n",
    "AdaDelta no manual tuning of a learning rate and \n",
    "appears robust to noisy gradient information, \n",
    "different model architecture choices, various data modalities \n",
    "and selection of hyperparameters. \n",
    "'''\n",
    "model.compile(optimizer=\"adadelta\", loss='mean_squared_error', metrics=['acc'])\n",
    "\n",
    "model.fit(padded_docs, target_train, epochs=18, verbose=1,batch_size=128)\n",
    "\n",
    "# testing for accuracy on test dataset\n",
    "print()\n",
    "loss, accuracy = model.evaluate(padded_docs_test, target_test, verbose=1)\n",
    "print(\"Accuracy: {0:.2f} %\".format(accuracy*100))\n",
    "\n",
    "# saving the model for future use\n",
    "try:\n",
    "    save = input(\"\\nSave Model? (yes=1, no=otherwise): \")\n",
    "    if(save=='1'):\n",
    "        name = input(\"Save as: \")\n",
    "        model_json = model.to_json()\n",
    "        with open(\"./models/{}.json\".format(name), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"./models/{}.h5\".format(name))\n",
    "        print(\"Saved {0}.json and {0}.h5 to disk\".format(name))\n",
    "    else:\n",
    "        print(\"Model not saved\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model to run to test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T17:26:08.210103Z",
     "start_time": "2018-12-30T17:26:05.190525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model: ['model', 'model_80.64', 'model_GPU_acc_80.54', 'model_acc_80.14']\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "models = !ls ./models\n",
    "mo = list()\n",
    "for m in models:\n",
    "    mod = m.rsplit('.', 1)[0]\n",
    "    if mod not in mo:\n",
    "        mo.append(mod)\n",
    "models = mo\n",
    "try:\n",
    "    name = input(\"Load model: {}\\n\".format(models))\n",
    "    if name not in models:\n",
    "        print(\"Model: {} not available. \\nLoading model: model\".format(name))\n",
    "        name = \"model\"     \n",
    "except:\n",
    "    print(\"Loading model: model\")\n",
    "    name = \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T17:26:08.370063Z",
     "start_time": "2018-12-30T17:26:08.214664Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "mwJg9XeyKSj2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model.json from disk\n",
      "Loaded model.h5 from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('./models/{}.json'.format(name), 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "print(\"Loaded {}.json from disk\".format(name))\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"./models/{}.h5\".format(name))\n",
    "print(\"Loaded {}.h5 from disk\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T17:26:08.585898Z",
     "start_time": "2018-12-30T17:26:08.375517Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "iaEXs_6zKU2s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992/992 [==============================] - 0s 106us/step\n",
      "model Accuracy: 81.25 %\n"
     ]
    }
   ],
   "source": [
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(padded_docs_test, target_test, verbose=1)\n",
    "print(\"{0} Accuracy: {1:.2f} %\".format(name ,(score[1]*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun-Phrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T17:26:08.613087Z",
     "start_time": "2018-12-30T17:26:08.588321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Today', 'great day', 'Indian politicians']]\n"
     ]
    }
   ],
   "source": [
    "text = ['Today is a very great day. Indian politicians are very corrupt']\n",
    "noun_phrases_eg = list()\n",
    "# list of stopwords from the english language\n",
    "stop_words = stopwords.words('english')\n",
    "# retrieving punctuations from string module\n",
    "punctuations = [i for i in string.punctuation]\n",
    "for sentence in text:\n",
    "    # \"per_noun_phrases\" contain all noun-phrases from each sentence\n",
    "    per_noun_phrases = list()\n",
    "    # removing all integers from sentences\n",
    "    sentence = re.sub(r'\\d+', \"\", sentence)\n",
    "    # taking care of residual spaces after digit removal\n",
    "    sentence = sentence.replace(\"  \", \" \")\n",
    "    # tokenizing the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # empty string \"np\" will take in individual noun-phrases\n",
    "    np = ''\n",
    "\n",
    "    for w in words:\n",
    "        if w not in stop_words+punctuations:\n",
    "            np += w + ' '\n",
    "        else:\n",
    "            if np != '':\n",
    "                per_noun_phrases.append(np.strip())\n",
    "                np = ''\n",
    "    noun_phrases_eg.append(per_noun_phrases)\n",
    "print(noun_phrases_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T17:26:08.647352Z",
     "start_time": "2018-12-30T17:26:08.615815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['look forward', 'meeting', 'learning', 'successful business'],\n",
       " ['look forward', 'seeing', 'next week']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_phrases_test[:2]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "StrideAI_Submission.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
